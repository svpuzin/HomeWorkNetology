# Домашнее задание к занятию «Микросервисы: подходы»

Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры.
Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.


## Задача 1: Обеспечить разработку

Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. 
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- облачная система;
- система контроля версий Git;
- репозиторий на каждый сервис;
- запуск сборки по событию из системы контроля версий;
- запуск сборки по кнопке с указанием параметров;
- возможность привязать настройки к каждой сборке;
- возможность создания шаблонов для различных конфигураций сборок;
- возможность безопасного хранения секретных данных (пароли, ключи доступа);
- несколько конфигураций для сборки из одного репозитория;
- кастомные шаги при сборке;
- собственные докер-образы для сборки проектов;
- возможность развернуть агентов сборки на собственных серверах;
- возможность параллельного запуска нескольких сборок;
- возможность параллельного запуска тестов.

Обоснуйте свой выбор.


## Ответ 

- Хранение исходного кода и система контроля версий: GitLab
- Непрерывная интеграция и непрерывная поставка (CI/CD): GitLab CI/CD
- Контейнеризация: Docker

GitLab в качестве ключевой платформы:
- Облачная инфраструктура: GitLab предлагает облачные возможности, что освобождает от необходимости администрирования собственных серверов.
- Управление версиями через Git: Полная поддержка Git с возможностью выделения отдельных репозиториев для каждого микросервиса.
- Изоляция сервисов: Каждый микросервис размещается в собственном репозитории GitLab, что гарантирует автономность и независимость процессов разработки.

Автоматизация через GitLab CI/CD:
- Активация сборки по событиям: Автоматический старт пайплайнов при push, создании merge request или других действиях в репозитории.
- Ручной запуск с настройками: Поддержка запуска пайплайнов вручную с передачей пользовательских параметров.
- Индивидуальные настройки: Применение файла .gitlab-ci.yml для детальной конфигурации каждого процесса сборки.
- Универсальные шаблоны: Использование YAML-шаблонов и механизмов включения для повторного применения готовых решений.
- Защищенное хранение данных: Секреты и конфиденциальная информация надежно хранятся в переменных окружения GitLab CI/CD.
- Многовариантные сборки: Настройка различных сценариев и окружений в рамках одного репозитория.
- Гибкость шагов: Возможность задавать произвольные скрипты и этапы в пайплайнах.
- Кастомные Docker-образы: Применение собственных Docker-образов для выполнения задач или в качестве исполнителей (runners).
- Локальные агенты: Развертывание GitLab Runners на собственной инфраструктуре для обработки сборок.
- Параллельная обработка: Одновременное выполнение множества задач и тестов с использованием параллельных джобов и динамических пайплайнов.


GitLab объединяет в себе систему контроля версий и инструменты CI/CD, что минимизирует сложности интеграции и упрощает управление процессами. Облачная модель обеспечивает высокую доступность и масштабируемость, а широкие возможности GitLab CI/CD полностью покрывают потребности в автоматизации, гибкости и безопасности разработки.




## Задача 2: Логи

Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
- минимальные требования к приложениям, сбор логов из stdout;
- гарантированная доставка логов до центрального хранилища;
- обеспечение поиска и фильтрации по записям логов;
- обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
- возможность дать ссылку на сохранённый поиск по записям логов.

Обоснуйте свой выбор.


## Ответ 

EFK-стек (ElasticSearch + Fluentd/Fluent Bit + Kibana)

Сервисы
- Приложения пишут логи только в stdout (минимальные требования).
- Контейнерная среда (Docker/Kubernetes) перенаправляет stdout/stderr в лог-файлы/драйверы.

Fluent Bit (или Fluentd)
- Лёгкий агент на каждом узле (node).
- Читает контейнерные логи (/var/lib/docker/containers/.../log.json или journald).
- Буферизует и гарантированно доставляет логи (с дисковым буфером).
- Отправляет в центральное хранилище (Elasticsearch).

Elasticsearch
- Централизованное хранилище логов.
- Поддерживает полнотекстовый поиск, индексацию по тегам/метаданным (сервис, контейнер, pod, namespace).
- Обеспечивает горизонтальное масштабирование (кластер).
- Гарантированная доставка — за счёт подтверждения записи от Elasticsearch и повторных попыток агента.

Kibana
- Веб-интерфейс для разработчиков/операторов.
- Поиск/фильтрация логов (по времени, сервису, severity, текстовым паттернам).
- Возможность сохранённого поиска и шаринга ссылок.
- Управление доступом через RBAC/LDAP/SSO.

Соответствие требованиям
- Сбор логов в центральное хранилище со всех хостов — агенты на каждом узле отправляют логи в Elasticsearch.
- Минимальные требования к приложениям, сбор из stdout — контейнерные драйверы + агенты читают stdout, приложение не меняется.
- Гарантированная доставка логов — дисковая буферизация агентов, подтверждение при записи в хранилище.
- Поиск и фильтрация по логам — Elasticsearch/Kibana.
- UI для разработчиков — Kibana.
- Ссылка на сохранённый поиск — Saved Search в Kibana.

## Задача 3: Мониторинг

Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор метрик со всех хостов, обслуживающих систему;
- сбор метрик состояния ресурсов хостов: CPU, RAM, HDD, Network;
- сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
- сбор метрик, специфичных для каждого сервиса;
- пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
- пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.

Обоснуйте свой выбор.


## Ответ 

Оптимальное решение — Prometheus + Node Exporter + cAdvisor (или kube-state-metrics) + Grafana.
Оно обеспечивает сбор всех необходимых метрик (инфраструктура, сервисы, специфичные показатели), хранение и анализ в централизованной системе, удобный интерфейс для разработчиков и возможность строить собственные панели и шарить запросы.

Архитектура решения
1. Сбор метрик с хостов (инфраструктура)
- На каждом узле разворачивается агент Node Exporter, который отдает метрики CPU, RAM, HDD, Network, процессы, загрузку.
- Prometheus периодически (pull-модель) опрашивает Node Exporter и сохраняет метрики в TSDB (time series DB).
2. Сбор метрик потребляемых ресурсов сервисов (per-service)
- Если используется Kubernetes: метрики ресурсов собирает kubelet + cAdvisor, доступные через kube-state-metrics.
- Для VM/docker окружений: можно использовать cAdvisor отдельно для контейнеров.
- Таким образом видно, сколько CPU/RAM/Disk/Net потребляет каждый pod/контейнер/сервис.
3. Сбор сервис-специфичных метрик
- Каждое приложение экспонирует endpoint /metrics в формате Prometheus (через библиотеки client SDK — для Go, Python, Java, .NET и т. д.).
4. Prometheus (или VictoriaMetrics/Thanos для масштабирования)
- Центральное хранилище временных рядов.
- Отвечает на PromQL-запросы для агрегаций и анализа.
- Поддерживает HA (двойные инстансы), federation для масштабирования, remote write.
5. Grafana (UI для пользователей)
- Подключается к Prometheus как источник данных.
- Позволяет строить гибкие дашборды: CPU/RAM/Disk/Net по хостам, нагрузка на сервисы, специфичные показатели.
- Позволяет делать ad-hoc запросы PromQL и строить агрегаты.
- Поддерживает шаринг ссылок на панели и запросы.
- Поддерживает RBAC, SSO для управления доступом разработчиков.

Соответствие требованиям
- Сбор метрик со всех хостов — Node Exporter.
- Сбор CPU, RAM, HDD, Network на хостах — Node Exporter.
- Сбор потребляемых ресурсов для каждого сервиса — cAdvisor/kube-state-metrics (для Kubernetes) или cAdvisor + Docker integration.
- Сбор специфичных метрик сервисов — endpoint /metrics с Prometheus SDK.
- UI для запросов и агрегаций — Grafana (PromQL запросы, Explore, панели).
- UI для настройки панелей — Grafana Dashboards (создание и сохранение собственных дашбордов).

Обоснование выбора
- Prometheus — стандарт де-факто в cloud-native мире, поддерживается Kubernetes, Docker, большинством сервисов.
- Node Exporter и cAdvisor — простые агенты, минимальные требования к хостам/сервисам, не нужно писать свой сборщик ресурсов.
- Prometheus SDK — позволяет быстро добавить бизнесовые/специфичные метрики в сервис.
- Grafana — мощный UI для визуализации и анализа: дашборды, фильтры, сохранённые запросы, доступ разработчикам.
- Масштабируемость: для больших систем можно использовать Thanos/VictoriaMetrics как backend для долгосрочного хранения и горизонтального масштабирования Prometheus.


## Задача 4: Логи * (необязательная)

Продолжить работу по задаче API Gateway: сервисы, используемые в задаче, пишут логи в stdout. 

Добавить в систему сервисы для сбора логов Vector + ElasticSearch + Kibana со всех сервисов, обеспечивающих работу API.

### Результат выполнения: 

docker compose файл, запустив который можно перейти по адресу http://localhost:8081, по которому доступна Kibana.
Логин в Kibana должен быть admin, пароль qwerty123456.


## Задача 5: Мониторинг * (необязательная)

Продолжить работу по задаче API Gateway: сервисы, используемые в задаче, предоставляют набор метрик в формате prometheus:

- сервис security по адресу /metrics,
- сервис uploader по адресу /metrics,
- сервис storage (minio) по адресу /minio/v2/metrics/cluster.

Добавить в систему сервисы для сбора метрик (Prometheus и Grafana) со всех сервисов, обеспечивающих работу API.
Построить в Graphana dashboard, показывающий распределение запросов по сервисам.

### Результат выполнения: 

docker compose файл, запустив который можно перейти по адресу http://localhost:8081, по которому доступна Grafana с настроенным Dashboard.
Логин в Grafana должен быть admin, пароль qwerty123456.

---

### Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.

---